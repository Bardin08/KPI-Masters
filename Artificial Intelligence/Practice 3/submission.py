import numpy as np

def parameters_inititalization(m):
  """
  Ця функція ініціалізує вектор-рядок випадкових дійсних значень ваг форми (1, m), отриманих з нормального розподілу та зсув (довільне дійсне значення)

  Параметри:
  m -- кількість вхідних ознак для кожного навчального прикладу

  Повертає:
  W -- вектор-рядок ваг форми (1, m)
  b -- зсув (скаляр)
  """

  # BEGIN_YOUR_CODE
  W = np.random.randn(1, m)
  b = 0.0

  return W, b
  # END_YOUR_CODE

def sigmoid(x):
  return 1 / (1 + np.exp(-x))

def forwardPropagate(X, W, b):
  """
  Ця функція обчислює лінійну комбінацію вхідних ознак та ваг, включаючи зсув і знаходить активаційне значення сигмоїди

  Параметри:
  X -- вхідна матриця форми (n_samples, n_features)
  W -- вектор-рядок ваг моделі форми (1, n_features)
  b -- зсув моделі (скаляр)

  Повертає:
  z -- загальна зважена сума вхідних ознак, включаючи зсув
  y_hat -- активаційне значення сигмоїди
  """

  # BEGIN_YOUR_CODE
  z = np.dot(X, W.T) + b
  y_hat = sigmoid(z.T)
  return z.T, y_hat
  # END_YOUR_CODE

def cost(n, y_hat, y_true):
  """
  Ця функція обчислює усереднену втрату для задачі бінарної класифікації на всьому навчальному наборі даних

  Параметри:
  n -- загальна кількість навчальних прикладів
  y_hat -- активаційне значення сигмоїди (прогноз логістичної регресії)
  y_true -- істинний клас зображення (очікувана мітка прогнозу)

  Повертає:
  J --  усереднена втрата моделі для задачі бінарної класифікації на всьому навчальному наборі даних
  """
  ep = 10E-10 # для уникнення в log(0)
  # BEGIN_YOUR_CODE
  y_hat = np.clip(y_hat, ep, 1 - ep)
  J = -(1/n) * np.sum(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))
  return J
  # END_YOUR_CODE

def backwardPropagate(n, X, y_hat, y_true):
  """
  Ця функція обчислює градієнти цільвої функції відносно ваг та зсуву

  Параметри:
  n -- загальна кількість навчальних прикладів
  X -- вхідна матриця форми (n_samples, n_features)
  y_hat -- активаційне значення сигмоїди (прогноз логістичної регресії)
  y_true -- істинний клас зображення (очікувана мітка прогнозу)

  Повертає:
  dW --  градієнт цільової функції відносно ваг моделі
  db -- градієнт цільової функції відносно зсуву моделі
  """

  # BEGIN_YOUR_CODE
  diff = y_hat - y_true

  dW = (1/n) * np.dot(diff, X)
  db = (1/n) * np.sum(diff)

  return dW, db
  # END_YOUR_CODE


def update(lr, dW, db, W, b):
    """
    Ця функція оновлює навчальні параметри моделі (ваги та зсув ) у напрямку мінімізації цільової функції

    Параметри:
    lr -- швидкість  навчання (крок навчання)
    dW --  градієнт цільової функції відносно ваг моделі
    db -- градієнт цільової функції відносно зсуву моделі
    W -- вектор-рядок ваг моделі форми (1, n_features)
    b -- зсув моделі (скаляр)

    Повертає:
    W -- оновлений вектор-рядок ваг моделі форми (1, n_features)
    b -- оновлений зсув моделі (скаляр)
    """

    # BEGIN_YOUR_CODE
    W = W - lr * dW
    b = b - lr * db

    return W, b
    # END_YOUR_CODE