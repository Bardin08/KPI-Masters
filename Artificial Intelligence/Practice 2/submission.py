import numpy as np

# Крок 0: Ініціалізувати ваги та зсув
def parameters_inititalization(m):
  """
  Ця функція ініціалізує вектор-рядок випадкових дійсних значень ваг форми (1, m),
  отриманих з нормального розподілу та зсув (довільне дійсне значення)

  Параметри:
  m -- кількість вхідних ознак для кожного навчального прикладу

  Повертає:
  W -- вектор-рядок ваг форми (1, m)
  b -- зсув (скаляр)
  """

  # BEGIN_YOUR_CODE
  W = np.random.randn(m, 1)
  b = 0

  return W, b
  # END_YOUR_CODE


# Крок 1: Обчислити лінійну комбінацію вхідних ознак та ваг, включаючи зсув
def forwardPropagate(X, W, b):
  """
  Ця функція обчислює лінійну комбінацію вхідних ознак та ваг, включаючи зсув

  Параметри:
  X -- вхідний вектор ознак форми (m, X_train.shape[1])
  W -- вектор-рядок ваг форми (1, m)
  b -- зсув моделі (скаляр)

  Повертає:
  z -- загальна зважена сума вхідних ознак, включаючи зсув
  y_hat -- прогноз моделі
  """

  # BEGIN_YOUR_CODE
  z = np.dot(W, X) + b
  y_hat = z
  return z, y_hat
  # END_YOUR_CODE


# Крок 2: Обчислити усереднену втрату на всьому навчальному наборі даних. Цільова функція
def cost(n, y_hat, y_true):
  """
  Ця функція обчислює середнє квадратичне відхилення на всьому навчальному наборі даних

  Параметри:
  n -- загальна кількість навчальних прикладів
  y_hat -- вихідне значення лінійної регресії
  y_true -- істинне значення залежної змінної

  Повертає:
  J -- середнє квадратичне відхилення на всьому навчальному наборі даних
  """

  # BEGIN_YOUR_CODE
  y_true = np.array(y_true).flatten()
  y_hat = np.array(y_hat).flatten()

  J = np.mean((y_hat - y_true) ** 2)

  return float(J)
  # END_YOUR_CODE


# Крок 3: Розрахувати градієнти цільвої функції відносно ваг та зсуву
def backwardPropagate(n, X, y_hat, y_true):
  """
  Ця функція обчислює градієнти цільвої функції відносно ваг та зсуву

  Параметри:
  n -- загальна кількість навчальних прикладів
  X -- вхідний вектор ознак форми (1, X_train.shape[1])
  y_hat --  вихідне значення лінійної регресії
  y_true -- істинне значення залежної змінної

  Повертає:
  dW --  градієнт цільової функції відносно ваг моделі
  db -- градієнт цільової функції відносно зсуву моделі
  """

  # BEGIN_YOUR_CODE

  # Різниця прогнозів та істинних значень
  diff = y_hat - y_true  # (y_hat^(i) - y^(i))

  # Обчислення градієнта відносно ваг
  # dJ/dW = (2/n) * sum((y_hat^(i) - y^(i)) * x^(i))
  dW = (2/n) * np.sum(diff * X)

  # Обчислення градієнта відносно зсуву
  # dJ/db = (2/n) * sum(y_hat^(i) - y^(i))
  db = (2/n) * np.sum(diff)

  return dW, db

  # END_YOUR_CODE


# Крок 4: Оновити ваги та зсув
def update(alpha, dW, db, W, b):
  """
  Ця функція оновлює навчальні параметри моделі (ваги та зсув ) у напрямку мінімізації цільової функції

  Параметри:
  alpha -- швидкість  навчання (крок навчання)
  dW --  градієнт цільової функції відносно ваг моделі
  db -- градієнт цільової функції відносно зсуву моделі
  W -- вектор-рядок ваг моделі форми (1, m)
  b -- зсув моделі (скаляр)

  Повертає:
  W -- оновлений вектор-рядок ваг моделі форми (1, m)
  b -- оновлений зсув моделі (скаляр)
  """


  # BEGIN_YOUR_CODE
  W = W - alpha * dW
  b = b - alpha * db
  return W, float(b)
  # END_YOUR_CODE
